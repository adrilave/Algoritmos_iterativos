---
title: "Untitled"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Iterative Algorithms

### Gradient Descent

Pseudo-code:
 
1. Start at a random location, parameters = random e.g x1 = 0, x2 = 0, x3 = 0.

2. Find the Gradient of the function I.e the first derivative respect to each of the components.

3. Establish a theta, which basically is the step size which the gradient is going to take, can be assigned as a parameter, for example theta = 0.01, or calculated using an approximation like Golden Search, Bisection Method, Newton's Method or using Wolfe rule (Backtracking Line Search) (All later developed).

4. Also establish a tolerance error to delimit how much error we want to accept.

5. Use the formula for Gradient Descent:
                
          (x1_k+1 x2_k+1 x3_k+1) = (x1_k x2_k x3_k) - theta * I * Gradient(f(X))

6. Iterate until reach the tolerance and then STOP.

```{r}
library(numDeriv)

initial_parameters <- c(0,-1)


objective_function <- function(x) {
  return(9*x[1]^2 + 2*x[1]*x[2] + x[2]^2)
}

gradient <- grad(objective_function, x= initial_parameters)

gradient_descent <- function(initial_parameters , error_margin = 1e-10, theta = 0.01){

  parameters <- initial_parameters
  x1 <- parameters[1]
  x2 <- parameters[2]
  #.... for more parameters
  
  while(sum(abs(gradient)) > error_margin){
    
    gradient <- grad(objective_function, x = parameters)
    
    new_parameters <- parameters - 0.0833 * gradient
    
    parameters <- new_parameters
    
    x1 <- parameters[1]
    x2 <- parameters[2]
    vector_parameters <- c(x1,x2)
    
  }
  return(round(vector_parameters,digits = 3))
}
gradient_descent(initial_parameters)
```

## Searching for the optimal step size

There are several methods for finding which step size to use in every iteration, if possible, the easiest form to find the best step size is just making the first derivative of the function respect to theta = 0, because is not always possible, we can use other methods such as:

- Golden Search (No need for derivative)

- Bisection Method (First derivative)

- Newton's Method (Second derivative)

- Backtracking Line Search (No need for derivative)

### Golden Search

Using the golden search we don't need to derive with respect to theta, acts between a known interval. 

```{r}

#If we want to maximize theta

objective_function_g_r <- function(x) {
  return(4*sin(x) * (1+cos(x)))
}

starting_interval <- c(0,pi/2)

golden_ratio = ((1+sqrt(5))/2)-1

f_a1 <- objective_function_g_r(starting_interval[1])
f_b1 <- objective_function_g_r(starting_interval[2])

golden_search_max <- function(starting_interval, error_margin = 1e-10){
  
  while(abs(f_a1-f_b1) > error_margin){

    a1 <- starting_interval[1] + (1-golden_ratio)*(starting_interval[2]-starting_interval[1])
    b1 <- starting_interval[2] - (1-golden_ratio)*(starting_interval[2]-starting_interval[1])

    f_a1 <- objective_function_g_r(a1)
    f_b1 <- objective_function_g_r(b1)
    
    if(f_a1 > f_b1){
      
    starting_interval[2] <- b1
    
    
    }
    if(f_a1 < f_b1){
      
    starting_interval[1] <- a1
  
    
    }
    
  }
  return(min(a1,b1))
}
golden_search_max(starting_interval)


#If we want to minimize theta

golden_search_min <- function(starting_interval, error_margin = 1e-10){
  
  while(abs(f_a1-f_b1) > error_margin){

    a1 <- starting_interval[1] + (1-golden_ratio)*(starting_interval[2]-starting_interval[1])
    b1 <- starting_interval[2] - (1-golden_ratio)*(starting_interval[2]-starting_interval[1])

    f_a1 <- objective_function_g_r(a1)
    f_b1 <- objective_function_g_r(b1)
    
    if(f_a1 > f_b1){
      
    starting_interval[1] <- a1
    
    
    }
    if(f_a1 < f_b1){
      
    starting_interval[2] <- b1
  
    
    }
    
  }
  return(min(a1,b1))
}
golden_search_min(starting_interval)

```

### Bisection Method

Uses the first derivative to find the root or roots of the function I.e f'(x) = 0.

Pseudo-code:

1. Find an interval where f'(a) < 0 & f'(b) > 0, then (a,b)

2. theta = a+b/2

3. If f'(theta) = 0 ; STOP, that is the optimal theta

4. If f'(theta) < 0 ; Minimum is in (theta, b) ; a = theta & b = b

5. If f'(theta) > 0 ; Minimum is in (a, theta) ; a = a & b = theta

6.Repeat until 3 or arrived to a error_margin

```{r}
objective_function_b_m <- function(x) {
  return(0.675 -13.28*x + 81.19*x^2)
}
bisection_example <- expression(0.675 -13.28*x + 81.19*x^2)
first_deriv_f_b_m <- D(expr = bisection_example,name = "x")


derivative_objective_function_b_m <- function(x) {
  return(81.19 * (2 * x) - 13.28)
}

interval <- c(0,1)

bisection_method <- function(interval, error_margin = 1e-10){
  
  a = interval[1]
  b = interval[2]
  
  while(abs(a - b) > error_margin){
    
    theta = 1/2*(a+b)
    
    derivative_objective_function_b_m(theta)
    print(a)
    print(b)
    
    if(derivative_objective_function_b_m(theta) < 0){
      
      a <- theta
      
    }else if(derivative_objective_function_b_m(theta) > 0){
      
      b <- theta
    
    }else{
      return(theta)
    }
  }
  return(theta)
}
bisection_method(interval)

```

### Newton's Method

Using Taylor Series to instead of finding the root using a linear approximation, uses a quadratic convergence, it does not guaranteed success, is needed to be near the minimum in the choose of theta.

                    theta_1 = theta_0 - f'(theta_0)/f''(theta_0)

Pseudo-code:

1. Find the first derivative and the second derivatives respect of theta.

2. Apply the Newton's Method formula until f'(theta) = 0.

```{r}
initial_expression <- expression(4 * sin(x) * (1 + cos(x)))

first_deriv_n_m <- D(initial_expression, name = "x")
first_deriv_n_m

first_derivative_objective_function <- function(x){
  return(4 * cos(x) * (1 + cos(x)) - 4 * sin(x) * sin(x))
}

first_derivative_expression <- expression(4 * cos(x) * (1 + cos(x)) - 4 * sin(x) * sin(x))

second_derivative_n_m <- D(first_derivative_expression, name = "x")
second_derivative_n_m

second_derivative_objective_function <- function(x){
  return(-(4 * cos(x) * sin(x) + 4 * sin(x) * (1 + cos(x)) + (4 * cos(x) * 
    sin(x) + 4 * sin(x) * cos(x))))
}

first_derivative_objective_function(pi/2)
second_derivative_objective_function(pi/2)
  
newton_method <- function(initial_theta, error_margin = 1e-10){
  
  theta <- 0
  
  while(abs(first_derivative_objective_function(theta)) > error_margin){
    
    theta <- initial_theta - (first_derivative_objective_function(initial_theta)/second_derivative_objective_function(initial_theta))
    
    initial_theta <- theta
    
  }
  return(theta)
}
newton_method(pi/2)
```



