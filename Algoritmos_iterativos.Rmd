---
title: "Untitled"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Iterative Algorithms

## Gradient Descent

Pseudo-code:
 
1. Start at a random location, parameters = random e.g x1 = 0, x2 = 0, x3 = 0.

2. Find the Gradient of the function I.e the first derivative respect to each of the components.

3. Establish a theta, which basically is the step size which the gradient is going to take, can be assigned as a parameter, for example theta = 0.01, or calculated using an approximation like Golden Search, Bisection Method, Newton's Method or using Wolfe rule (Backtracking Line Search) (All later developed).

4. Also establish a tolerance error to delimit how much error we want to accept.

5. Use the formula for Gradient Descent:
                
          (x1_k+1 x2_k+1 x3_k+1) = (x1_k x2_k x3_k) - theta * I * Gradient(f(X))

6. Iterate until reach the tolerance and then STOP.

```{r}
library(numDeriv)

initial_parameters <- c(0,-1)


objective_function <- function(x) {
  return(9*x[1]^2 + 2*x[1]*x[2] + x[2]^2)
}

gradient_descent <- function(initial_parameters , error_margin = 1e-10, theta = 0.01){

  parameters <- initial_parameters
  x1 <- parameters[1]
  x2 <- parameters[2]
  #.... for more parameters
  
  while(sum(abs(gradient)) > error_margin){
    
    gradient <- grad(objective_function, x = parameters)
    
    new_parameters <- parameters - 0.0833 * gradient
    
    parameters <- new_parameters
    
    x1 <- parameters[1]
    x2 <- parameters[2]
    vector_parameters <- c(x1,x2)
    
  }
  return(round(vector_parameters,digits = 3))
}
gradient_descent(initial_parameters)
    

```





